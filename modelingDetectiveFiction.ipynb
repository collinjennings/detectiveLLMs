{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed48eceb",
   "metadata": {},
   "source": [
    "# Detective Story Experiment \n",
    "8.27.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "436d85e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import transformerModelFunctions\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import textwrap\n",
    "wrapper = textwrap.TextWrapper(width=70)\n",
    "tf.keras.utils.set_random_seed(10)\n",
    "import glob\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53057af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: keras 3.5.0\n",
      "Uninstalling keras-3.5.0:\n",
      "  Would remove:\n",
      "    /Applications/anaconda3/lib/python3.9/site-packages/keras-3.5.0.dist-info/*\n",
      "    /Applications/anaconda3/lib/python3.9/site-packages/keras/*\n",
      "Proceed (Y/n)? ^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436f0a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages('keras3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90bc62b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"~/Documents/GitHub/BMDS/BMDS_story_annotations.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ee3f437",
   "metadata": {},
   "outputs": [],
   "source": [
    "metaDF = pd.read_csv(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffba5688",
   "metadata": {},
   "outputs": [],
   "source": [
    "metaList =[]\n",
    "for index, row in metaDF.iterrows(): \n",
    "    metaList.append({'code': row['Story Code'], 'title': row['Story Title'], 'author': row['Author Code'], 'key clue': row['Essential clue'], \n",
    "                     'structure': row['Investigation-reveal order'], 'reveal line': row['Reveal border sentence']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e698f658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'No': 385, 'Yes': 35, nan: 15})\n",
      "Counter({'No': 264, 'Yes': 156, nan: 15})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(metaDF['Sufficient clues to solve?']))\n",
    "print(Counter(metaDF['Correct annotator guess?']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4583e446",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"/Users/collinjennings/Documents/GitHub/BMDS/texts/*.txt\")\n",
    "texts = defaultdict()\n",
    "\n",
    "for afile in files: \n",
    "    texts[re.sub('_','', afile.split('.')[0].split('/')[7])] = open(afile, encoding = 'utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3959155f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Yes': 250, 'No': 170, nan: 15})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(metaDF['Sufficient clues to guess?']) #['Sufficient Clues to Guess?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f342062",
   "metadata": {},
   "outputs": [],
   "source": [
    "clueTextList = [] \n",
    "for item in metaList:\n",
    "    if item['key clue'] != '_none' and item['code'] in texts.keys(): \n",
    "        item['text'] = '[SOS]'+texts[item['code']]+'[EOS]'\n",
    "        item['key clue'] = '[SOS]'+item['key clue']+'[EOS]'\n",
    "        clueTextList.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c736b7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clueTextList[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1045c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "413"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clueTextList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf203157",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(clueTextList[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4336fc14",
   "metadata": {},
   "source": [
    "## Steps\n",
    "1. Organize the metadata as a list of dictionaries. \n",
    "2. Process texts as lists of sentences stored as word lists. \n",
    "3. Create lists of essential clues and pair with appropriate story texts. \n",
    "4. Match the \"essential clue\" with a specific appearance in the story full text. \n",
    "5. Mark the sentence in which the reveal begins. \n",
    "\n",
    "I think I want to run 2 versions of the experiment: one in which I give the classifier the full story, and one in which I make it rely on the story up to the point of the reveal. I would expect that in the full story, the classifier can look for the most repeated words/phrases to identify the key clue. \n",
    "\n",
    "**Designing the Experiment**\n",
    "I'm trying to think of a model for this kind of experiment since I'm not doing a two or multi class experiment. I'm asking the model to find the same kind of term/phrase in each story. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1710a5ba",
   "metadata": {},
   "source": [
    "## Creating the Detective Model\n",
    "\n",
    "Here I'm thinking about the \"essential clue\" as a kind of summary about the document.\n",
    "\n",
    "Tweaks: \n",
    "- add [SOS] and [EOS] to text and clues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66be154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(len(clueTextList)*.8)\n",
    "trainList = clueTextList[:split]\n",
    "testList = clueTextList[split:]\n",
    "#random.shuffle(trainList)\n",
    "#random.shuffle(testList)\n",
    "#print(trainList[-1])\n",
    "#print(testList[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70f607c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame.from_records(trainList, columns=['key clue', 'text'])\n",
    "test_data = pd.DataFrame.from_records(testList, columns=['key clue', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76542929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "key clue    [SOS]The handwriting samples demanded by the c...\n",
       "text        [SOS]\\nShortly after my marriage I had bought ...\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dectDF.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a116a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Modify to use my created corpus\n",
    "\n",
    "# Take one example from the dataset and print it\n",
    "example_summary, example_dialogue = train_data.iloc[10]\n",
    "print(f\"Dialogue:\\n{example_dialogue}\")\n",
    "print(f\"\\nSummary:\\n{example_summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30186b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09eb6d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Need to input data as strings. \n",
    "document, summary = transformerModelFunctions.preprocess(train_data)\n",
    "document_test, summary_test = transformerModelFunctions.preprocess(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c56be35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 53552\n"
     ]
    }
   ],
   "source": [
    "# The [ and ] from default tokens cannot be removed, because they mark the SOS and EOS token.\n",
    "## Clue descriptions will replace `summary` here. \n",
    "\n",
    "filters = '!\"#$%&()*+,-./:;<=>?@\\\\^_`{|}~\\t\\n'\n",
    "oov_token = '[UNK]'\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token, lower=False)\n",
    "\n",
    "documents_and_summary = pd.concat([document, summary], ignore_index=True)\n",
    "\n",
    "tokenizer.fit_on_texts(documents_and_summary)\n",
    "\n",
    "inputs = tokenizer.texts_to_sequences(document)\n",
    "targets = tokenizer.texts_to_sequences(summary)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "print(f'Size of vocabulary: {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9375022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit the size of the input and output data for being able to run it in this environment.\n",
    "encoder_maxlen = 150\n",
    "decoder_maxlen = 50\n",
    "\n",
    "# Pad the sequences.\n",
    "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
    "targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=decoder_maxlen, padding='post', truncating='post')\n",
    "\n",
    "inputs = tf.cast(inputs, dtype=tf.int32)\n",
    "targets = tf.cast(targets, dtype=tf.int32)\n",
    "\n",
    "# Create the final training dataset.\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d620c068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model parameters\n",
    "num_layers = 2\n",
    "embedding_dim = 128\n",
    "fully_connected_dim = 128\n",
    "num_heads = 2\n",
    "positional_encoding_length = 256\n",
    "\n",
    "# Initialize the model\n",
    "transformer = transformerModelFunctions.Transformer(\n",
    "    num_layers, \n",
    "    embedding_dim, \n",
    "    num_heads, \n",
    "    fully_connected_dim,\n",
    "    vocab_size, \n",
    "    vocab_size, \n",
    "    positional_encoding_length, \n",
    "    positional_encoding_length,\n",
    ")\n",
    "\n",
    "learning_rate = transformerModelFunctions.CustomSchedule(embedding_dim)\n",
    "optimizer = tf.keras.optimizers.Adam(0.0002, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "# Here you will store the losses, so you can later plot them\n",
    "losses = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc55b5a1",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a90b9f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1/6\r"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/collinjennings/Dropbox/NJMove/detectiveStories/transformerModelFunctions.py\", line 533, in train_step  *\n        predictions, _ = model(\n    File \"/Applications/anaconda3/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Applications/anaconda3/lib/python3.9/site-packages/keras/src/layers/layer.py\", line 808, in __call__\n        raise ValueError(\n\n    ValueError: Only input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: True (of type <class 'bool'>)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/74/l_mwgvjn0974lx2y1svywt000000gn/T/ipykernel_22534/2790544753.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch+1}, Batch {batch+1}/{number_of_batches}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtransformerModelFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch+1}, Loss {train_loss.result():.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/NJMove/detectiveStories/transformerModelFunctions.py\u001b[0m in \u001b[0;36mtf__train_step\u001b[0;34m(model, inp, tar)\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mdec_padding_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                     \u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar_inp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlook_ahead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar_real\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/keras/src/layers/layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m                     \u001b[0;32mand\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m                 ):\n\u001b[0;32m--> 808\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    809\u001b[0m                         \u001b[0;34m\"Only input tensors may be passed as \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m                         \u001b[0;34m\"positional arguments. The following argument value \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/collinjennings/Dropbox/NJMove/detectiveStories/transformerModelFunctions.py\", line 533, in train_step  *\n        predictions, _ = model(\n    File \"/Applications/anaconda3/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Applications/anaconda3/lib/python3.9/site-packages/keras/src/layers/layer.py\", line 808, in __call__\n        raise ValueError(\n\n    ValueError: Only input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: True (of type <class 'bool'>)\n"
     ]
    }
   ],
   "source": [
    "# Take an example from the test set, to monitor it during training\n",
    "test_example = 0\n",
    "true_summary = summary_test[test_example]\n",
    "true_document = document_test[test_example]\n",
    "\n",
    "# Define the number of epochs\n",
    "epochs = 20\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    start = time.time()\n",
    "    train_loss.reset_state()\n",
    "    number_of_batches=len(list(enumerate(dataset)))\n",
    "\n",
    "    for (batch, (inp, tar)) in enumerate(dataset):\n",
    "        print(f'Epoch {epoch+1}, Batch {batch+1}/{number_of_batches}', end='\\r')\n",
    "        transformerModelFunctions.train_step(transformer, inp, tar)\n",
    "    \n",
    "    print (f'Epoch {epoch+1}, Loss {train_loss.result():.4f}')\n",
    "    losses.append(train_loss.result())\n",
    "    \n",
    "    print (f'Time taken for one epoch: {time.time() - start} sec')\n",
    "    print('Example summarization on the test set:')\n",
    "    print('  True summarization:')\n",
    "    print(f'    {true_summary}')\n",
    "    print('  Predicted summarization:')\n",
    "    print(f'    {summarize(transformer, true_document)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0bfad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e82b180",
   "metadata": {},
   "source": [
    "## Solve the stories!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fc20de",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_example = 0\n",
    "\n",
    "# Check a summary of a document from the training set\n",
    "print('Training set example:')\n",
    "print(document[training_set_example])\n",
    "print('\\nHuman written summary:')\n",
    "print(summary[training_set_example])\n",
    "print('\\nModel written summary:')\n",
    "print(summarize(transformer, document[training_set_example]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bf8382",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_example = 3\n",
    "\n",
    "# Check a summary of a document from the test set\n",
    "print('Test set example:')\n",
    "print(document_test[test_set_example])\n",
    "print('\\nHuman written summary:')\n",
    "print(summary_test[test_set_example])\n",
    "print('\\nModel written summary:')\n",
    "print(summarize(transformer, document_test[test_set_example]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f2b589",
   "metadata": {},
   "source": [
    "## Measure the accuracy of the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
