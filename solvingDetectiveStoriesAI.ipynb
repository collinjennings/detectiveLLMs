{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/collinjennings/detectiveLLMs/blob/main/solvingDetectiveStoriesAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_otNpud6mQQ1"
      },
      "source": [
        "# Predicting Detective Story Culprits with Gemini\n",
        "This is trial code for predicting culprits in detective short stories. I'm developing the process for doing this at the corpus level and comparing results to reader predictions and annotations.\n",
        "\n",
        "The code was written to be deployed in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TbSGAkYmPBJ",
        "outputId": "721332d8-ba30-4cd0-e29f-317686fedaee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/6.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/6.2 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m4.4/6.2 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --user --quiet google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DBSZwB4DU6NK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbnX5xGfmXkM",
        "outputId": "6ad239aa-e56b-41a9-b931-5a67bfdbd87b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXV5drzbo0RS"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import torch\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'\n",
        "device = torch.device('cuda')\n",
        "#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:1000\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PAJB82ymlNN"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Additional authentication is required for Google Colab\n",
        "if \"google.colab\" in sys.modules:\n",
        "    # Authenticate user to Google Cloud\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZOFS882pDgv",
        "outputId": "c4f95cfc-8ca5-45d8-9298-dcdda6308d5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd '/content/drive/MyDrive/Colab Notebooks'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsdgc-F3qZmT"
      },
      "outputs": [],
      "source": [
        "# Define project information\n",
        "PROJECT_ID = \"#####\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-east1\"  # @param {type:\"string\"}\n",
        "\n",
        "# Initialize Vertex AI\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdRgiw4rrgEN"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "from vertexai.generative_models import (\n",
        "    GenerationConfig,\n",
        "    GenerativeModel,\n",
        "    HarmBlockThreshold,\n",
        "    HarmCategory,\n",
        "    Part,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qclPtMSn9j0"
      },
      "source": [
        "### Process Story Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6_eP38PrtFQ"
      },
      "outputs": [],
      "source": [
        "model = GenerativeModel(\n",
        "    \"gemini-1.5-flash\",\n",
        "    safety_settings={\n",
        "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
        "    },\n",
        ")\n",
        "# This Generation Config sets the model to respond in JSON format.\n",
        "generation_config = GenerationConfig(\n",
        "    temperature=0.0, response_mime_type=\"application/json\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KreRGbYn4n9"
      },
      "outputs": [],
      "source": [
        "## Loading the corpus.\n",
        "files = glob.glob(\"./data/texts/*.txt\")\n",
        "texts = defaultdict()\n",
        "for afile in files:\n",
        "      texts[re.sub('_','', afile.split('/')[3].split('.')[0])] = open(afile, encoding = 'utf-8').read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqTmF-oquBGg"
      },
      "outputs": [],
      "source": [
        "## Loading the metadata\n",
        "metaList = []\n",
        "fname = './data/BMDS_story_annotations.csv' ### Update filename\n",
        "metaDF = pd.read_csv(fname)\n",
        "for index, row in metaDF.iterrows():\n",
        "  metaList.append({'code': row['Story Code'], 'title': row['Story Title'], 'author': row['Author Code'], 'solvability1': row['Sufficient clues to guess?'],\n",
        "                   'solvability2': row['Sufficient clues to solve?'], 'solvability3': row['Correct annotator guess?'],\n",
        "                   'key clue1': row['Essential clue'], 'key clue2': row['Most salient clue'], 'Solved?': row['Is the crime solved?'],\n",
        "                   'structure': row['Investigation-reveal order'], 'reveal line': row['Reveal border sentence']})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FvzS5KNudjA"
      },
      "outputs": [],
      "source": [
        "### Filter out stories that aren't solved\n",
        "print(len(metaList))\n",
        "for i in metaList:\n",
        "  if i['Solved?'] == 'No':\n",
        "    metaList.remove(i)\n",
        "print(len(metaList))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Crop stories before the reveal"
      ],
      "metadata": {
        "id": "xmE_xOWRTenV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jplSOQq2u7e0"
      },
      "outputs": [],
      "source": [
        "for id, i in enumerate(metaList[:6]):\n",
        "  if i['key clue1'][:8].lower() not in i['key clue2'].lower():\n",
        "    i['clues'] = [i['key clue1'], i['key clue2']]\n",
        "  else:\n",
        "    i['clues'] = [i['key clue1']]\n",
        "  text = texts[i['code']]\n",
        "  print(len(text.split()))\n",
        "  i['text'] = text[:text.find(i['reveal line'][:18])]\n",
        "  print(len(i['text'].split()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSrBEIb98hds"
      },
      "source": [
        "### Few shot prompting\n",
        "Give the LLM a prompt and several examples to help make its predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-B-pJkm6Yqn"
      },
      "outputs": [],
      "source": [
        "qa_prompt = \"\"\"Read the excerpt from a detective story, and predict who committed the crime.\n",
        "The perpetrator is not identified in this portion of the story. The story will try to trick you with red herring clues.\n",
        "Make sure to predict something that has not already happened. Then identify one key piece of evidence (consider material and testimonial clues) that supports your prediction.\"\"\"\n",
        "\n",
        "text = ''\n",
        "prompt = f\"\"\" Given the fields \"Context\" and \"Question\", produce the fields \"Prediction\" and \"Evidence\".\n",
        "---\n",
        "\"Context\": {metaList[5]['text']},\n",
        "\"Question\": {qa_prompt},\n",
        "\"Prediction\": The perpetrator is the trainer James Ryder who later uses the alias John Robinson.\n",
        "\"Evidence\": The key clue is his interest in the goose that Mr. Baker received, which Holmes discovers by finding the goose supplier by his wager with Mr. Breckinridge.\n",
        "\n",
        "---\n",
        "\"Context\": {metaList[1]['text']},\n",
        "\"Question\": {qa_prompt},\n",
        "\"Prediction\": Arthur Pinner, the brother of Harry Pinner, is the perpetrator. He likely orchestrated the entire scheme to defraud\n",
        "the firm Pycroft was hired to.\n",
        "\"Evidence\": The fact that both brothers have the same gold-filled tooth is a strong indicator that they are not who they claim\n",
        "to be. This suggests a deliberate attempt to deceive Hall Pycroft.\n",
        "\n",
        "---\n",
        "\"Context\": {metaList[3]['text']},\n",
        "\"Question\": {qa_prompt},\n",
        "\"Prediction\": Sir George Burnwell is the perpetrator. Arthur took the blame to protect Mary.\n",
        "\"Evidence\": The footprints in the snow and Mary suddenly closing the window indicate that someone else was involved in trying to take the beryls.\n",
        "\n",
        "---\n",
        "\"Context\": {metaList[0]['text']},\n",
        "\"Question\": {qa_prompt},\n",
        "\"Prediction\":\n",
        "\"Evidence\":\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Print model prediction\n",
        "response = model.generate_content(prompt, generation_config=generation_config).text\n",
        "print(f\"Answer: {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXI95W6TFMkN",
        "outputId": "12d9dc65-ad63-4d87-d241-cf59d4287885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: {\"Prediction\": \"The perpetrator is Fitzroy Simpson. He likely killed Straker to cover up his theft of Silver Blaze.\", \"Evidence\": \"The key clue is the fact that Fitzroy Simpson's cravat was found in the dead man's hand. This suggests a struggle between the two, and that Simpson was present at the scene of the crime.\"}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6gUk+5MgLE3k1wyrM1B/X",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}